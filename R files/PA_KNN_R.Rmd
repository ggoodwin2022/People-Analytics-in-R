---
title: "Predicting Employee Performance with KNN Algo"
output: github_document
---

#### People Analytics Applications in *R*
##### K-Nearest Neighbors Classifier

***

This tutorial will illustrate how to use the **K-Nearest Neighbors Classifier** in *R* using the *caret* package (Kuhn, 2021). In this example, our goal is to use a number or HR-related variables to predict which performance group new employees are likely to fall under. 

* Dataset and codebook can be found on my [github page](https://github.com/ggoodwin2022/People-Analytics-in-R)

**NOTE**: If you're already familiar with EDA & data viz, you can skip ahead to the **Modeling Pipeline** section

***

#### Data Import & Set-Up

```{r setup, message = FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(readr)
library(readxl)
library(caret)
library(janitor)
library(DescTools)
library(RColorBrewer)
library(patchwork)
library(sjPlot)
library(GGally)

options(scipen = 999)

hr1 <- read_csv("HR_Sample_Data.csv") # Read in Data

hr1[-c(1,6,7,13)] -> hr1 # Remove predictors we don't need

hr1 %>% 
  map_lgl(is.character) -> char_cols # Identifies Character Variables

hr1[char_cols] %>% 
  map_df(as.factor) -> hr1[char_cols] # Converts to Factors

hr1$salary <- round(hr1$salary/1000, digits = 0) # Salary in Thousands Units
```

***

#### Exploratory Data Analysis

Before we begin the modeling process, we need to first take a look at our dataset to understand its structure, examine important characteristics such as data missingness, and identify any relevant trends or relationships.

##### Structure & Descriptive Statistics

```{r}
view_df(hr1,show.type = T,
        show.na = T, 
        show.frq = T, 
        show.prc = T, 
        show.values = T)
```

```{r}
summary(hr1)
```

***

**Some important characteristics to look for:**

* **Data Missingness**
  * we have 0% missingness on all variables
* **Frequency/Distribution**
  * Marital Desc variable may present a challenge
    * Freqs are very uneven across factor levels
  * Our Performance Rating outcome has a 90:10 ratio
    * This is okay, but be wary of very uneven outcome classes, especially when sample size is low
* **Data Types & Structure**
  * All of our categorical & numeric variables are properly coded as such

***

##### Data Visualization

###### Univariate Plots

The code below selects all the numeric variables in your dataset, pivots to long form, and then creates a ggplot faceted by each variable type. I highly recommend running the code one line at a time to see how it works. Note that the **geom_density** can be changed.

```{r}
hr1 %>%
  select_if(is.numeric) %>% 
  pivot_longer(names_to = "variable",
               values_to = "value",
               cols = 1:5) %>% 
  ggplot(aes(x = value,
             color = variable,
             fill = variable)) +
  facet_wrap(~ variable, scales = "free") +
  geom_density(alpha = 0.5)
```


###### Multivariate Plots

For modeling relationships amongst our variables, the **GGally** package has a fantastic function called **ggpairs**, shown below.

```{r}
hr1 %>% 
  select(c(salary,
           engage_survey,
           emp_satisfaction,
           performance_rating)) %>% 
  ggpairs(aes(colour = performance_rating, alpha = 0.4))
```

We can also recreate the same plots ourselves:
```{r}
ggplot(data = hr1, 
       aes(x = engage_survey,
           y = salary,
           color = performance_rating)) +
  geom_point(size = 0.9) +
  geom_jitter(size = 0.9) +
  scale_color_brewer(palette = "Set1") +
  theme_bw() + 
  labs(title = "Salary & Engagement Level by Performance Rating")
```


```{r}
ggplot(data = hr1, 
       aes(x = engage_survey,
           y = salary,
           color = performance_rating)) +
  geom_point(size = 0.9) +
  geom_jitter(size = 0.9) +
  facet_wrap(~performance_rating) +
  scale_color_brewer(palette = "Set1") +
  theme_bw() + 
  labs(title = "Salary & Engagement Level by Performance Rating")
```


```{r}
ggplot(data = hr1, 
       aes(x = department,
           fill = performance_rating)) +
  geom_bar(alpha = 0.7) +
  theme_bw() +
  labs(title = "Stacked Bar Chart of Performance Group by Department")
```

```{r}
ggplot(data = hr1, 
       aes(x = salary,
           fill = performance_rating)) +
  geom_boxplot(alpha = 0.5) +
  theme_bw() +
  labs(title = "Boxplot of Salaries by Performance Group")
```

***

#### Modeling Pipeline

With the *caret* package, are a TON of customization options for your modeling pipeline. We're going to stick to the simpler, more generalizable pipeline methods, but just be aware there are many ways you can tweak this as desired.

***

##### Partition Data into Train & Test

We'll start by using caret's **createDataPartition** to split our main dataset into a training and testing set

* Set seed for reproducibility
* Stratify w/respect to the outcome via (y = hr1$performance_rating)
* Specify the split ratio via (p = training %) 
  * 70:30 split used below

```{r}
set.seed(825) # Set Seed

# Create index for split
## Specify outcome Y, split ratio, and say list = FALSE

index_trn <- createDataPartition(y = hr1$performance_rating, 
                                 p = .7, 
                                 list = FALSE,
                                 times = 1)

# Use index to create train & test subsets
hr_trn <- hr1[index_trn,]
hr_test <- hr1[-index_trn,]

```

***

##### Hyperparameter Tuning

We use **trnControl** to specify our hyperparameter **tuning method**. There are a LOT of different options, but a simple default is to use **repeated k-fold cross-validation**.

For the KNN model, we need to tune for the optimal selection of **k**. We'll do so using an accuracy-based 10-fold cross-validation repeated 3x

```{r}

# Specify method, number of folds, and number of repeats
trn_Control <- trainControl(method = "repeatedcv",
                            number = 10,
                            repeats = 3)
```

***

##### Data Pre-Processing 

We have a couple of data pre-processing steps we need to carry out.

* **Create Dummy Vars for our categorical predictors**
  * e.g., Convert a 5-level categorical variable to 4 dummy vars
* **Normalize/rescale our predictors** 
  * VERY important w/KNN or any other distance-based algorithms
  * If neglected, larger-scale predictors will have undue influence

Good news! The *caret* package allows us to pre-process our data, tune our hyperparameters, and train our model all in the same step!

We'll see how this works below in the model training section

***

##### Model Training

We use *caret's* **train** function to create our fitted model.

* Specify *formula* 
  * **Y ~ . ** if all predictors included
  * (Y = salary + engage + ...) if specific predictors included
* Specify *training data*
  * **data** = hr_trn for this example
* Specify *model family* via model method
  * **method** = "knn" for KNN
  * different models have different tags to specify
    * **names(getModelInfo())** will give a list of all models
* Specify *tuning method* using the method name created earlier
  * **trControl** = trn_Control
* Specify *pre-processing* steps
  * We want **preProcess** == **center** & **scale**

**IMPORTANT**: Using formula notation will lead *caret* to **automatically** convert your factor variables to dummies internally. We don't need to do anything to explicitly convert categorical to dummies.

After fitting your model, printing the fitted object will give an overview of the model selected, along with the optimal hyperparameter values chosen 

```{r}

# Create fitted model
knn_fit1 <- train(performance_rating ~ ., 
                  data = hr_trn,
                  method = "knn",
                  trControl = trn_Control,
                  preProcess = c("center", "scale"),
                  tuneLength = 20)

knn_fit1 # Overview


```

We can also plot the hyperparameter tuning process:

```{r}
ggplot(knn_fit1) # Plot Hyperparameter tuning 
```

We can also get a summary of the *final model*. This is kind of pointless for KNN, but is very useful for algorithms like the RandomForest, where we might want to see what the final decision rules were:

```{r}
knn_fit1$finalModel
```

***

##### Evaluating Predictive Performance

Now that we've fit out model, it's time to evaluate it's **predictive performance**. First we need to generate predictions using our test set.

###### Generating Predictions for Test Set

* Specify the **fitted object** = fitted model name
* Specify the **newdata** = testing set name
* Specify the **type** of prediction (raw or probabilistic)

First we'll generate **probabilistic predictions**, where we're given a probability estimate for each outcome class

```{r}
# Probabilistic Preds first
knn_pred_prob1 <- predict(knn_fit1,
                          newdata = hr_test,
                          type="prob")

head(knn_pred_prob1)
```

Next we'll generate "raw" predictions, or **fitted values**, where we get a simple class membership prediction rather than a probabilistic estimate.

```{r}
# "Raw"/Fitted Preds Second
knn_pred_fitted1<-predict(knn_fit1, 
                          newdata=hr_test,
                          type = "raw")

head(knn_pred_fitted1)
```

###### Evaluating Performance

Next we'll compare the predictions generated to the **actual** test set performance group values. 

* It only makes sense to do this for the "raw" predictions
  * Specify **pred** = predictions generated
  * Specify **obs** = actual test set outcome values
  
```{r}
# Evaluate Predictive Performance upon Test Set
postResample(pred = knn_pred_fitted1, 
             obs = hr_test$performance_rating)
```

We see that when used upon our testing set, our KNN model yields an accuracy of ~92%. Not bad!


We can also generate a **Confusion Matrix**

* Specify **data** = fitted prediction object
* Specify **reference** = actual test set outcome values

```{r}
# Confusion Matrix
confusionMatrix(data = knn_pred_fitted1,
                reference = hr_test$performance_rating)
```

It looks like our model does a really good job of predicting the high-performers, but not so great at predicting the low-performers.

***

#### Generating Predictions for *Future* Data

It's great that our model performs well on our test set, but now that we know that, we want to use our **fitted model** to predict **new** data. Fortunately, the process is the exact same, and simply involves using *R's* **predict** function again.

We don't have future data, so let's create a subset of our data and pretend its new.

```{r}
hr1[1:50,] -> hr_new   # subset pretending to be new data for tutorial

hr_new[-7] -> hr_new # Remove performance outcome from "new" data

```

Predicting performance classes for these "new" data points is simple!

* Specify **fitted model object**
* Specify **new data** to predict

```{r}
predict(knn_fit1, newdata = hr_new) -> new_preds

head(new_preds)
```

***

##### Recap

We've covered the following steps:

* **Exploratory Data Analysis**
  * Data Structure
  * Data Viz
* **Data Pre-Processing**
  * Center & Scale/Normalize
* **Data Partitioning**
  * 70:30 Train/Test Split
* **Hyperparameter Tuning**
  * Selecting optimal value of *k*
* **Model Fitting/Training**
  * Create fitted model object
* **Predictive Performance**
  * Generate test set performance predictions
  * Compare predictions to actual outcomes
  * Create Confusion Matrix to slice/dice accuracy issues
* **Predict New/Future Data**
  * Generate predictions from fitted model using *predict* function
  
***

Thanks for reading! Please feel free to reach out with any questions