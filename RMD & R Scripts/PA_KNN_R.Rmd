---
title: "Predicting Employee Performance with KNN Algo"
output: github_document
---

#### People Analytics Applications in *R*
##### K-Nearest Neighbors Classifier

***

This tutorial will illustrate how to use the **K-Nearest Neighbors Classifier** in *R* using the *caret* package (Kuhn, 2021). In this example, our goal is to use a number or HR-related variables to predict which performance group new employees are likely to fall under. 

* Dataset and codebook can be found on my [github page](https://github.com/ggoodwin2022/People-Analytics-in-R)

**NOTE**: If you're already familiar with EDA & data viz, you can skip ahead to the **Modeling Pipeline** section

***

#### Data Import & Set-Up

```{r setup, message = FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(readr)
library(readxl)
library(caret)
library(janitor)
library(DescTools)
library(RColorBrewer)
library(patchwork)
library(sjPlot)
library(GGally)

options(scipen = 999)

hr1 <- read_csv("HR_Sample_Data.csv") # Read in Data

hr1[-c(1,6,7,13)] -> hr1 # Remove predictors we don't need

hr1 %>% 
  map_lgl(is.character) -> char_cols # Identifies Character Variables

hr1[char_cols] %>% 
  map_df(as.factor) -> hr1[char_cols] # Converts to Factors

hr1$salary <- round(hr1$salary/1000, digits = 0) # Salary in Thousands Units
```

***

#### Exploratory Data Analysis

Before we begin the modeling process, we need to first take a look at our dataset to understand its structure, examine important characteristics such as data missingness, and identify any relevant trends or relationships.

##### Structure & Descriptive Statistics

```{r}
view_df(hr1,show.type = T,
        show.na = T, 
        show.frq = T, 
        show.prc = T, 
        show.values = T)
```

```{r}
summary(hr1)
```

***

**Some important characteristics to look for:**

* **Data Missingness**
  * we have 0% missingness on all variables
* **Frequency/Distribution**
  * Marital Desc variable may present a challenge
    * Freqs are very uneven across factor levels
  * Our Performance Rating outcome has a 90:10 ratio
    * This is okay, but be wary of very uneven outcome classes, especially when sample size is low
* **Data Types & Structure**
  * All of our categorical & numeric variables are properly coded as such

***

##### Data Visualization

###### Univariate Plots

The code below selects all the numeric variables in your dataset, pivots to long form, and then creates a ggplot faceted by each variable type. I highly recommend running the code one line at a time to see how it works. Note that the **geom_density** can be changed.

```{r}
hr1 %>%
  select_if(is.numeric) %>% 
  pivot_longer(names_to = "variable",
               values_to = "value",
               cols = 1:5) %>% 
  ggplot(aes(x = value,
             color = variable,
             fill = variable)) +
  facet_wrap(~ variable, scales = "free") +
  geom_density(alpha = 0.5)
```


###### Multivariate Plots

For modeling relationships amongst our variables, the **GGally** package has a fantastic function called **ggpairs**, shown below.

```{r}
hr1 %>% 
  select(c(salary,
           engage_survey,
           emp_satisfaction,
           performance_rating)) %>% 
  ggpairs(aes(colour = performance_rating, alpha = 0.4))
```

We can also recreate the same plots ourselves:
```{r}
ggplot(data = hr1, 
       aes(x = engage_survey,
           y = salary,
           color = performance_rating)) +
  geom_point(size = 0.9) +
  geom_jitter(size = 0.9) +
  scale_color_brewer(palette = "Set1") +
  theme_bw() + 
  labs(title = "Salary & Engagement Level by Performance Rating")
```


```{r}
ggplot(data = hr1, 
       aes(x = engage_survey,
           y = salary,
           color = performance_rating)) +
  geom_point(size = 0.9) +
  geom_jitter(size = 0.9) +
  facet_wrap(~performance_rating) +
  scale_color_brewer(palette = "Set1") +
  theme_bw() + 
  labs(title = "Salary & Engagement Level by Performance Rating")
```


```{r}
ggplot(data = hr1, 
       aes(x = department,
           fill = performance_rating)) +
  geom_bar(alpha = 0.7) +
  theme_bw() +
  labs(title = "Stacked Bar Chart of Performance Group by Department")
```

```{r}
ggplot(data = hr1, 
       aes(x = salary,
           fill = performance_rating)) +
  geom_boxplot(alpha = 0.5) +
  theme_bw() +
  labs(title = "Boxplot of Salaries by Performance Group")
```

***

#### Modeling Pipeline

With the *caret* package, there are a few different ways we can approach our modeling pipeline. Because this is a tutorial, I'm going to show the longer version where each step is explicitly carried out one at a time.

***

##### Data Pre-Processing

We have a couple of data pre-processing steps we need to carry out.

* Create Dummy Vars for our categorical predictors
  * e.g., Convert a 5-level categorical variable to 4 dummy vars
* Normalize/scale our predictors so they are all on the same scale
  * VERY important w/KNN or any other distance-based algorithms


###### Design Matrix/Data Frame including Categorical Vars

In this step we'll create a feature/design matrix where all our variables are represented numerically.

```{r}
# Categorical to Dummy via Base R Model Matrix
model.matrix(performance_rating ~ ., 
             data = hr1) -> X_mat

X_df <- data.frame(X_mat[,-1]) # Convert matrix to DF, remove Intercept

head(X_df)

```


###### Center & Scale/Normalize Predictors

Now we need to rescale all of our predictors so that variables with larger scales don't have undue influence. We'll do this via **preProcess**.

Note that the **preProcess** function doesn't actually *transform* our predictors yet, it just establishes the transformation method. It also gives a summary of which variables were impacted:

```{r}
# Center and Scale


preproc_X <- preProcess(x = X_df,
                       method = c("center", "scale"))

preproc_X
```

We use the **predict** function to actually "predict" the transformed values using our established tranformation method:
```{r}
X_scaled <- predict(preproc_X, newdata = X_df) # TRANSFORM   


# Append performance outcome back onto Feature matrix
hr1_scaled <- cbind(hr1["performance_rating"],X_scaled)

hr1_scaled %>% 
  relocate(performance_rating,
           .before = salary) -> hr1_scaled

head(hr1_scaled)
```

***

##### Data Partition into Train & Test

We'll use caret's **createDataPartition** to split our main dataset into a training and testing set

* Set seed for reproducibility
* Stratify w/respect to the outcome (tell it what "y" is)
* Specify the split ratio (p = training %, 70:30 split used below)

```{r}
set.seed(825) # Set Seed

# Create index for split
index_trn <- createDataPartition(y = hr1_scaled$performance_rating, 
                                 p = .7, 
                                 list = FALSE,
                                 times = 1)

# Use index to create train & test subsets
hr_trn <- hr1_scaled[index_trn,]
hr_test <- hr1_scaled[-index_trn,]

```

***

##### Hyperparameter Tuning

The *caret* package's **trnControl** function allows you to choose the manner in which you want your hyperparameters to be tuned. There are a LOT of different customization options, but a simple default is to use **repeated k-fold cross-validation**.

For the KNN model, we need to tune for the optimal selection of **k**. We'll do so using an accuracy-based 10-fold cross-validation repeated 3x

```{r}

# We'll use 10-fold CV repeated 3x
trn_Control <- trainControl(method = "repeatedcv",
                            number = 10,
                            repeats = 3)
```

It's important to note that as with the **preProcess** step, the tuning doesn't actually occur via **trainControl**, that's just how we *specify* the *tuning method* The actual tuning is conducted alongside the model training step.

***

##### Model Training

We use *caret's* **train** function to create our fitted model.

* Specify formula 
  * **Y ~ . ** if all predictors included
  * (Y = salary + engage + ...) if specific predictors included
* Specify training data
  * **data** = hr_trn for this example
* Specify model family via model method
  * **method** = "knn" for KNN
  * different models have different tags to specify
    * **names(getModelInfo())** will give a list of all models
* Specify name of **tuning method** created above
  * **trControl** = trn_Control

After fitting your model, printing the fitted object will give an overview of the model selected, along with the optimal hyperparameter values chosen 

```{r}

# Create fitted model
knn_fit1 <- train(performance_rating ~ ., 
                  data = hr_trn,
                  method = "knn", 
                  trControl = trn_Control,
                  tuneLength = 20)

knn_fit1 # Overview


```

We can also plot the hyperparameter tuning process:

```{r}
ggplot(knn_fit1) # Plot Hyperparameter tuning 
```

We can also get a summary of the *final model*. This is kind of pointless for KNN, but is very useful for algorithms like the RandomForest, where we might want to see what the final decision rules were:

```{r}
knn_fit1$finalModel
```

Finally, we can examine **variable importance**:

```{r}
varImp(knn_fit1)
```

***

##### Evaluating Predictive Performance

Now that we've fit out model, it's time to evaluate it's **predictive performance**. First we need to generate predictions using our test set.

###### Generating Predictions for Test Set

* Specify the **fitted object** = fitted model name
* Specify the **newdata** = testing set name
* Specify the **type** of prediction (raw or probabilistic)

First we'll generate **probabilistic predictions**, where we're given a probability estimate for each outcome class

```{r}
# Probabilistic Preds first
knn_pred_prob1 <- predict(knn_fit1, 
                       newdata = hr_test, 
                       type="prob")

head(knn_pred_prob1)
```

Next we'll generate "raw" predictions, or **fitted values**, where we get a simple class membership prediction rather than a probabilistic estimate.

```{r}
# "Raw"/Fitted Preds Second
knn_pred_fitted1<-predict(knn_fit1, 
                          newdata=hr_test,
                          type = "raw")

head(knn_pred_fitted1)
```

###### Evaluating Performance

Next we'll compare the predictions generated to the **actual** test set performance group values. 

* It only makes sense to do this for the "raw" predictions
  * Specify **pred** = predictions generated
  * Specify **obs** = actual test set outcome values
  
```{r}
# Evaluate Predictive Performance upon Test Set
postResample(pred = knn_pred_fitted1, 
             obs = hr_test$performance_rating)
```

We see that when used upon our testing set, our KNN model yields an accuracy of ~92%. Not bad!


We can also generate a **Confusion Matrix**

* Specify **data** = fitted prediction object
* Specify **reference** = actual test set outcome values

```{r}
# Confusion Matrix
confusionMatrix(data = knn_pred_fitted1,
                reference = hr_test$performance_rating)
```

It looks like our model does a really good job of predicting the high-performers, but not so great at predicting the low-performers.

***

##### Recap

We've covered the following steps:

* **Exploratory Data Analysis**
  * Data Structure
  * Data Viz
* **Data Pre-Processing**
  * Center & Scale/Normalize
* **Data Partitioning**
  * 70:30 Train/Test Split
* **Hyperparameter Tuning**
  * Selecting optimal value of *k*
* **Model Fitting/Training**
  * Create fitted model object
  
6) **Predictive Performance**

  * Generate test set performance predictions
  * Compare predictions to actual outcomes
  * Create Confusion Matrix to slice/dice accuracy issues

***

Thanks for reading! Please feel free to reach out with any questions
